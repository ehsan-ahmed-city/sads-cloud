# sads-cloud
Secure and Auditable Data Storage in the Cloud

# Github repo:
https://github.com/ehsan-ahmed-city/sads-cloud


## Overview
SADS-Cloud is a cloud-based secure data outsourcing system that demonstrates how sensitive data can be stored,processed and retrieved in the cloud without fully trusting the cloud provider.

The project follows the original proposal and research foundations while extending them with practical cloud-native features such as behaviour-based trust monitoring, encrypted data clustering and indexing

The system is implemented using AWS services and is a Streamlit-based app.


## Architecture Summary

The system is split into three main components:

### 1 Authentication/ Trust Centre  
Asad Arshad

- AWS cognito-based authentication
- Immutable login event logging in amazon S3
- Behaviour based suspicious login detection by requirements i made based on paper
- Serverless alerting using AWS lambda and SNS
- Aggressive detection to minimise false negatives



### 2 Encryption upload and compression pipeline and EMR attempt  
Ehsan Ahmed

- Client-side LZMA compression
- Chunk-based SALSA20 encryption
- Secure upload to S3
- Attempted parallel processing through EMR serverless, with logging but is limited to RAW file uplaod as beyond that couldn not be done because of dependencies such asubuntu
- Helped wiring functions to streamlit UI


### 3 Storage, Clustering and Indexing  
**Author:** Zain Butt

- DBSCAN clustering on encrypted metadata
- Fractal Index Tree for fast lookup and retrieval
- Cost-aware access by minimising S3 scans
- Secure reconstruction and integrity verification



## Cloud usage justification

Quik note that screenshots of AWS services are in /awsScreenshots for proof; the services only display on AWS console so screenshots were uploaded to show it

This project uses cloud services because they are necessary, the technologies we used here were::

- **Amazon S3**: used for storage as it's a durable,scalable object storage for encrypted data and audit logs  
- **AWS Cognito**: created users for all of us, placed our users in a group, managed authentication and token verification  
- **AWS Lambda**: serverless alerting for sus behaviour.
- **AWS SNS**: real-time email notifications which is sent to selected emails such as an admin 
- **EMR Serverless**: big data style batch processing (but limited to only raw file supload).
-  **AWS IAM**: IAM was used to setup permissions for user to access data and setting up


## IAM and Security Configuration

Since our project makes heavy use of AWS services, IAM configuration was performed directly in AWS Console for security practice and to avoid committing sensitive infra or credentials to the repo

### IAM Roles Overview

IAM roles are separated by responsibility, since there's 3 of us we created a user group called SADS-Cloud-Dev with permission policies whcih enabled us ot interact with aws services

---

### 1. Lambda Execution Role for suspicious login alerts

`sads-lambda-suspicious-login-role`

**what it does:**  
Allows the Lambda function to process login events generated by the Trust Centre and publish alerts when suspicious behaviour is detected
(/awsScreenshots/lambda-suspicious-login.png)

permissions:
- `s3:GetObject`  
  Read immutable login log objects from  
  `sads-raw-data/trust-centre/logins/`
- `sns:Publish`  
  send alert notifications to the `sads-suspicious-login`

- `logs:CreateLogGroup`, `logs:CreateLogStream`, `logs:PutLogEvents`  
  the event logs to amazon CloudWatch

**Reason used:**  
This role is minimal scope to event processing and alerting only, so it cannot modify logs or access unrelated S3 objects

---

### 2. EMR Serverless Execution Role

**what it does:**  
Allows EMR serverless jobs to perform parallel encryption and storage of uploaded data

Permissions:
- `s3:GetObject` on `raw/`  
- `s3:PutObject` on `encrypted/`  
- CloudWatch logging perms

**Reason used:**  
well ir was supposed to support batch-style encryption whih can scale for big data while ensuring EMR jobs can only read raw input data and write encrypted outputs. There's no direct access to unrelated buckets/prefixes



## Cognito for users identity and ownership in S3

**What it does:**  
AWS Cognito is used to manage user identities for the system and to show data owners and data users. Like when the email sns for the login came through, the data owner recieved it.
/awsScreenshots/cogniUser.png

Each authenticated user is goven a unique cognito user ID, which is then used consistently across the system to scope storage, track behaviour and enforce what they can access

**How it used in the project:**
- Test users are created and managed in an AWS Cognito user pool
- On login, the Cognito access token is verified by the application
- The extracted `user_id` becomes the identifier for the storage paths (like encrypted,raw,fit,cluster), immutable login audit logs, behaviour analysis

The s3 model: 
Each user only interacts with objects under their own prefix in the `sads-raw-data` bucket, shown in the dmeo but an example would be user1234: 'Buckets/sads-raw-data/encrypted/user1234/'

This models common multi-user cloud environment where users do not share storage namespaces

Security properties provided:
- Prevents users from accessing or overwriting other users’ data
- Allows auditability and anomaly detection and users can be identified

**Reason used:**  
Cognito allows identity management to be handled by a managed cloud service, separating authentication concerns from data processing logic. This aligns with the project’s trust model and enables a realistic, secure cloud-based demonstration without implementing a custom authentication system.


### IAM Design choice

IAM roles are intentionally separated across the following responsibilities:
- Authentication  
- Data processing  
- alerting  

which improves:
- security as theres minimal permissions per component  
- Audit logs as there's clear ownership of actions  
- Isolates faults becauase compromised components cannot escalate privileges  

This design aligns with the project’s trust model, where cloud components are assumed to be honest and are therefore tightly constrained through IAM policies.


Cost and resource usage are minimised by:
- Chunked encryption
- Metadata-only clustering
- Indexed retrieval instead of full scans
- Serverless components where appropriate

---

### EMR serverless big-data encryption attempt
awsScreenshots/emrDefunc.png

So from the proposal, the project explored whether encryption and storage operations could be offloaded to support large-scale workloads

To develop this, we created an EMR serverless Spark app to accept RAW file inputs S3, perform batch-style encryption and then write encrypted outputs back to S3
which is then logged.

The EMR workflow successfully supports job submission, scheduling, logging, and RAW file upload, as shown in the one of the screenshots(`awsScreenshots/emrDefunc.png`).

However, during execution, the job encounters limitations due to native linux/Ubuntu dependency constraints within the EMR. This prevented certain libs from loading correctly without unmanaged clusters

Rather than introducing insecure or non-portable workarounds, this limitation was documented in a commit and the system retains a **secure client-side encryption pipeline** as the primary function of what we based our solution from our research paper.

**Result:**  
EMR Serverless was used to evaluate scalability, cost and feasibility of big-data encryption rather than as a mandatory way of execution but the results showed the final design decisions and sticking to the principle of waht was in the paper


### AWS Lambda forlogin alert

The screenshot `awsScreenshots/lambda-suspicious-login.png` shows the deployed AWS Lambda function which implements the serverless alerting component of the trust centre

It's automatically triggered by S3 object creation events in `trust-centre/logins/`. (a screenshot in awsScreenshots/s3LoginBucketDates) Each object represents an immutable login event written by the
authentication layer.

The function: reads newly created login log objects from S3
- parses the login decision produced by the anomaly detection logic
- If the login marked as suspicious, publishes an alert message to AWS SNS
- If the login is normal, nothing else happens

The screenshot demonstrates:
- The Lambda function deployed and active in AWS
- Integration with S3 as an event source
- The serverless nature of the alerting pipeline (no persistent infrastructure)

**Why Lambda is used:**
Lambda allows suspicious login alerts to be handled **asynchronously and automatically**, without
adding latency to the authentication flow. This matches the project’s trust model, where monitoring
and enforcement are decoupled from user-facing operations.

**Security and design properties:**
- The function runs under a **least-privilege IAM role**
- It has read-only access to login logs
- It cannot modify data or access encrypted user content
- Alert delivery is handled via SNS, ensuring reliability and auditability

This Lambda-based design demonstrates a realistic cloud security pattern used in production systems,
where detection, alerting, and response are implemented as independent serverless components.


## Research

The project is based on research into secure cloud data outsourcing and trust management systems.
This ncludes:
- Behaviour based login anomaly detection such as based on custom rules
- Serverless alerting pipeline
- Practical encrypted data clustering
- Cost-aware indexing and retrieval, saves time and resources
- End-to-end system with audit logging

Supporting research documents are included separately.



## Demo

The project demo vid includes a Streamlit console that showed:
- user authentication for verified users(managed in cognito)
- file upload and encryption
- clustering and indexing
- secure retrieval and integrity verification
- monitoring of cloud-side artefacts in S3

## Contributors

- **Asad Arshad** — Authentication, Trust Centre, anomaly detection and  AWS Lambda, alerting 
- **Ehsan Ahmed** — AWS user setup in cognito, AWS IAM, Encryption, compression, cloud operations integrating, UI integration  
- **Zain Butt** — Clustering, indexing, lookup, retrieval optimisation  


